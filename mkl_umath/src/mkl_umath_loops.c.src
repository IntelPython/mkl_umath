/*
 * Copyright (c) 2019, Intel Corporation
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 *     * Redistributions of source code must retain the above copyright notice,
 *       this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of Intel Corporation nor the names of its contributors
 *       may be used to endorse or promote products derived from this software
 *       without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/* -*- c -*- */
#include "mkl.h"
#include <float.h>
#include <fenv.h>
#include "Python.h"

#define NPY_NO_DEPRECATED_API NPY_API_VERSION
#define NP_IMPORT_ARRAY

#include "numpy/npy_common.h"
#include "numpy/ndarraytypes.h"
#include "numpy/ndarrayobject.h"
#include "numpy/ufuncobject.h"
#include "numpy/npy_math.h"
#include "blocking_utils.h"
#include "mkl_umath_loops.h"

// NPY_2_0_API_VERSION 0x00000012 is defined in numpy-2
// inside numpy/_core/include/numpy/numpyconfig.h
#if NPY_API_VERSION >= 0x00000012 
    #define USE_NUMPY_2
#endif

/* Adapted from NumPy's source code.
 * https://github.com/numpy/numpy/blob/main/LICENSE.txt */

/*
 * largest simd vector size in bytes numpy supports
 * it is currently a extremely large value as it is only used for memory
 * overlap checks
 */
#ifndef NPY_MAX_SIMD_SIZE
#define NPY_MAX_SIMD_SIZE 1024
#endif

/*
 * cutoff blocksize for pairwise summation
 * decreasing it decreases errors slightly as more pairs are summed but
 * also lowers performance, as the inner loop is unrolled eight times it is
 * effectively 16
 */
#define PW_BLOCKSIZE    128
#define VML_TRANSCEDENTAL_THRESHOLD 8192
#define VML_ASM_THRESHOLD 100000
#define VML_D_THRESHOLD 8000

#define MKL_INT_MAX ((npy_intp) ((~((MKL_UINT) 0)) >> 1))

#define CHUNKED_VML_CALL2(vml_func, n, type, mkl_type, mkl_op_type, in1, op1)   \
    do {                                                                        \
        npy_intp _n_ = (n);                                                     \
        const npy_intp _chunk_size = MKL_INT_MAX;                               \
        type *in1p = (type *) (in1);                                            \
        type *op1p = (type *) (op1);                                            \
        while (_n_ > 0) {                                                       \
            npy_intp _current_chunk = (_n_ > _chunk_size) ? _chunk_size : _n_;  \
            vml_func((MKL_INT) _current_chunk, (mkl_type *)(void *) in1p,       \
                     (mkl_op_type *)(void *) op1p);                             \
            _n_ -= _current_chunk;                                              \
            in1p += _current_chunk;                                             \
            op1p += _current_chunk;                                             \
        }                                                                       \
    } while (0)

#define CHUNKED_VML_CALL3(vml_func, n, type, mkl_type, in1, in2, op1)           \
    do  {                                                                       \
        npy_intp _n_ = (n);                                                     \
        const npy_intp _chunk_size = MKL_INT_MAX;                               \
        type *in1p = (type *) (in1);                                            \
        type *in2p = (type *) (in2);                                            \
        type *op1p = (type *) (op1);                                            \
        while (_n_ > 0) {                                                       \
            npy_intp _current_chunk = (_n_ > _chunk_size) ? _chunk_size : _n_;  \
            vml_func((MKL_INT) _current_chunk, (mkl_type *)(void *) in1p,       \
                     (mkl_type *)(void *) in2p, (mkl_type *)(void *) op1p);     \
            _n_ -= _current_chunk;                                              \
            in1p += _current_chunk;                                             \
            in2p += _current_chunk;                                             \
            op1p += _current_chunk;                                             \
        }                                                                       \
    } while(0)


#define CHUNKED_VML_LINEARFRAC_CALL(vml_func, n, type, in1, op1, scaleA, shiftA, scaleB, shiftB) \
     do {                                                                                        \
        npy_intp _n_ = (n);                                                                      \
        const npy_intp _chunk_size = MKL_INT_MAX;                                                \
        type *in1p = (type *) (in1);                                                             \
        type *op1p = (type *) (op1);                                                             \
        const type _scaleA = (scaleA);                                                           \
        const type _shiftA = (shiftA);                                                           \
        const type _scaleB = (scaleB);                                                           \
        const type _shiftB = (shiftB);                                                           \
        while (_n_ > 0) {                                                                        \
            npy_intp _current_chunk = (_n_ > _chunk_size) ? _chunk_size : _n_;                   \
            vml_func(_current_chunk, in1p, in1p, _scaleA, _shiftA, _scaleB, _shiftB, op1p);      \
            _n_ -= _current_chunk;                                                               \
            in1p += _current_chunk;                                                              \
            op1p += _current_chunk;                                                              \
        }                                                                                        \
    } while(0)

/* for pointers p1, and p2 pointing at contiguous arrays n-elements of size s, are arrays disjoint or same
 *  when these conditions are not met VML functions may produce incorrect output
 */
#define DISJOINT_OR_SAME(p1, p2, n, s) (((p1) == (p2)) || ((p2) + (n)*(s) < (p1)) || ((p1) + (n)*(s) < (p2)))
#define DISJOINT_OR_SAME_TWO_DTYPES(p1, p2, n, s1, s2) (((p1) == (p2)) || ((p2) + (n)*(s2) < (p1)) || ((p1) + (n)*(s1) < (p2)))

/*
 * include vectorized functions and dispatchers
 * this file is safe to include also for generic builds
 * platform specific instructions are either masked via the proprocessor or
 * runtime detected
 */

/** Provides the various *_LOOP macros */
#include "fast_loop_macros.h"
#include <stdio.h>

static inline npy_double spacing(npy_double x) {
    if (isinf(x))
    return ((npy_double) NAN);
    return copysign(nextafter(fabs(x), ((npy_double) INFINITY)), x) - x;
}

static inline npy_float spacingf(npy_float x) {
    if (isinff(x))
    return ((npy_float) NAN);

    return copysignf(nextafterf(fabsf(x), INFINITY), x) - x;
}

#if defined(_MSC_VER) && defined(__INTEL_COMPILER)
extern __inline float __cdecl ldexpf( float _X, int _Y) { 
    return (float)ldexp(_X, _Y); 
}
#endif

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #c = f, #
 */
/*
 * Python version of divmod.
 *
 * The implementation is mostly copied from cpython 3.5.
 */
static inline @type@
divmod@c@(@type@ a, @type@ b, @type@ *modulus)
{
    @type@ div, mod, floordiv;

    mod = fmod@c@(a, b);

    if (!b) {
        /* If b == 0, return result of fmod. For IEEE is nan */
        *modulus = mod;
        return mod;
    }

    /* a - mod should be very nearly an integer multiple of b */
    div = (a - mod) / b;

    /* adjust fmod result to conform to Python convention of remainder */
    if (mod) {
        if ((b < 0) != (mod < 0)) {
            mod += b;
            div -= 1.0@c@;
        }
    }
    else {
        /* if mod is zero ensure correct sign */
        mod = copysign@c@(0, b);
    }

    /* snap quotient to nearest integral value */
    if (div) {
        floordiv = floor@c@(div);
        if (div - floordiv > 0.5@c@)
            floordiv += 1.0@c@;
    }
    else {
        /* if div is zero ensure correct sign */
        floordiv = copysign@c@(0, a/b);
    }

    *modulus = mod;
    return floordiv;
}
/**end repeat**/

/*
 *****************************************************************************
 **                             FLOAT LOOPS                                 **
 *****************************************************************************
 */
/* TODO: Use MKL for pow, arctan2, fmod, hypot, i0 */

/**begin repeat
 * Float types
 * #type = npy_float, npy_double#
 * #TYPE = FLOAT, DOUBLE#
 * #c = f, # 
 * #s = s, d#
 */

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
#if defined(__ICC) || defined(__INTEL_COMPILER)
#ifdef _MSC_VER
#pragma intel optimization_level 1
#else
#pragma intel optimization_level 2
#endif
#endif
static @type@
pairwise_sum_@TYPE@(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        /*
         * Start with -0 to preserve -0 values. The reason is that summing
         * only -0 should return -0, but `0 + -0 == 0` while `-0 + -0 == -0`.
         */
        @type@ res = -0.0;

        for (i = 0; i < n; i++) {
            res += (*((@type@*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        @type@ r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = (*((@type@ *)(a + 0 * stride)));
        r[1] = (*((@type@ *)(a + 1 * stride)));
        r[2] = (*((@type@ *)(a + 2 * stride)));
        r[3] = (*((@type@ *)(a + 3 * stride)));
        r[4] = (*((@type@ *)(a + 4 * stride)));
        r[5] = (*((@type@ *)(a + 5 * stride)));
        r[6] = (*((@type@ *)(a + 6 * stride)));
        r[7] = (*((@type@ *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(@type@))*stride, 0, 3);
            r[0] += (*((@type@ *)(a + (i + 0) * stride)));
            r[1] += (*((@type@ *)(a + (i + 1) * stride)));
            r[2] += (*((@type@ *)(a + (i + 2) * stride)));
            r[3] += (*((@type@ *)(a + (i + 3) * stride)));
            r[4] += (*((@type@ *)(a + (i + 4) * stride)));
            r[5] += (*((@type@ *)(a + (i + 5) * stride)));
            r[6] += (*((@type@ *)(a + (i + 6) * stride)));
            r[7] += (*((@type@ *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += (*((@type@ *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return pairwise_sum_@TYPE@(a, n2, stride) +
               pairwise_sum_@TYPE@(a + n2 * stride, n - n2, stride);
    }
}

/**begin repeat1
 * # kind = add, subtract#
 * # OP = +, -#
 * # PW = 1, 0#
 * # VML = Add, Sub#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));

    if (IS_BINARY_CONT(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same1 && disjoint_or_same2) {
            CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, @type@, args[0], args[1], args[2]);
            /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;

                    if (DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
                        DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1)) {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        NPY_PRAGMA_VECTOR
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
                        }
                    } 
                    else {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
                        }
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }
        }
    } 
    else if (IS_BINARY_CONT_S1(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same2) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[1], args[2], @OP@1.0, *(@type@*)args[0], 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[1], (@type@*) args[1], @OP@1.0, *(@type@*)args[0], 0.0, 1.0, (@type@*) args[2]); */
        } 
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip1c = ip1[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip2_aligned = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

                    NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1c @OP@ ip2_aligned[j];
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }
        }
    } 
    else if (IS_BINARY_CONT_S2(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same1) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[0], args[2], 1.0, @OP@(*(@type@*)args[1]), 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[0], (@type@*) args[0], 1.0, @OP@(*(@type@*)args[1]), 0.0, 1.0, (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip2c = ip2[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip1_aligned = ip1 + peel;

                    NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1_aligned[j] @OP@ ip2c;
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }
        }
    } 
    else if (IS_BINARY_REDUCE) {
#if @PW@
        @type@ * iop1 = (@type@ *)args[0];
        npy_intp n = dimensions[0];

        *iop1 @OP@= pairwise_sum_@TYPE@(args[1], n, steps[1]);
#else
        BINARY_REDUCE_LOOP(@type@) {
            io1 @OP@= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
#endif
    } 
    else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_multiply(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));

    if (IS_BINARY_CONT(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same1 && disjoint_or_same2) {
            CHUNKED_VML_CALL3(v@s@Mul, dimensions[0], @type@, @type@, args[0], args[1], args[2]);
            /* v@s@Mul(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] * ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;

                    if ( DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
                        DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1)) {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        NPY_PRAGMA_VECTOR
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] * ip2_shifted[j];
                        }
                    } 
                    else {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] * ip2_shifted[j];
                        }
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] * ip2[i];
            }
        }
    } 
    else if (IS_BINARY_CONT_S1(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same2) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[1], args[2], *(@type@*)args[0], 0.0, 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[1], (@type@*) args[1], *(@type@*)args[0], 0.0, 0.0, 1.0, (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip1c = ip1[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1c * ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip2_aligned = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

                    NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1c * ip2_aligned[j];
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1c * ip2[i];
            }
        }
    } 
    else if (IS_BINARY_CONT_S2(@type@, @type@)) {
        if (dimensions[0] > VML_ASM_THRESHOLD && disjoint_or_same1) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[0], args[2], *(@type@*)args[1], 0.0, 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[0], (@type@*) args[0], *(@type@*)args[1], 0.0, 0.0, 1.0, (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip2c = ip2[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] * ip2c;
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip1_aligned = ip1 + peel;

                    NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1_aligned[j] * ip2c;
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] * ip2c;
            }
        }
    } 
    else if (IS_BINARY_REDUCE) {
        BINARY_REDUCE_LOOP(@type@) {
            io1 *= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
    } 
    else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 * in2;
        }
    }
}

void
mkl_umath_@TYPE@_divide(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));

    if (IS_BINARY_CONT(@type@, @type@)) {
        if (dimensions[0] > VML_D_THRESHOLD && disjoint_or_same1 && disjoint_or_same2) {
            CHUNKED_VML_CALL3(v@s@Div, dimensions[0], @type@, @type@, args[0], args[1], args[2]);
            /* v@s@Div(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        }
        else {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

        NPY_PRAGMA_NOVECTOR
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] / ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
        j_max &= (~0xf);
        const npy_intp blocked_end = j_max + peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;

                    if (DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
                        DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1)) {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        NPY_PRAGMA_VECTOR
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] / ip2_shifted[j];
                        }
                    } 
                    else {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] / ip2_shifted[j];
                        }
                    }

                    i = blocked_end;
                }
            }

        NPY_PRAGMA_NOVECTOR
            for(; i < n; i++) {
                op1[i] = ip1[i] / ip2[i];
            }
        }
    } 
    else if (IS_BINARY_CONT_S1(@type@, @type@)) {
        @type@ *ip1 = (@type@*)args[0];
        @type@ *ip2 = (@type@*)args[1];
        @type@ *op1 = (@type@*)args[2];
        const npy_intp vsize = 64;
        const npy_intp n = dimensions[0];
        const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
        const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
        npy_intp i;

        const @type@ ip1c = ip1[0];
    NPY_PRAGMA_NOVECTOR
        for(i = 0; i < peel; i++) {
            op1[i] = ip1c / ip2[i];
        }

        {
            npy_intp j, j_max = blocked_end - peel;
            if (j_max > 0) {
                @type@ *ip2_aligned = ip2 + peel, *op1_shifted = op1 + peel;

                NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                for(j = 0; j < j_max; j++) {
                    op1_shifted[j] = ip1c / ip2_aligned[j];
                }

                i = blocked_end;
            }
        }

    NPY_PRAGMA_NOVECTOR
        for(; i < n; i++) {
            op1[i] = ip1c / ip2[i];
        }
    } 
    else if (IS_BINARY_CONT_S2(@type@, @type@)) {
        @type@ *ip1 = (@type@*)args[0];
        @type@ *ip2 = (@type@*)args[1];
        @type@ *op1 = (@type@*)args[2];
        const npy_intp vsize = 64;
        const npy_intp n = dimensions[0];
        const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
        const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
        npy_intp i;

        const @type@ ip2c = ip2[0];
    NPY_PRAGMA_NOVECTOR
        for(i = 0; i < peel; i++) {
            op1[i] = ip1[i] / ip2c;
        }

        {
            npy_intp j, j_max = blocked_end - peel;
            if (j_max > 0) {
                @type@ *ip1_aligned = ip1 + peel, *op1_shifted = op1 + peel;

                NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                for(j = 0; j < j_max; j++) {
                    op1_shifted[j] = ip1_aligned[j] / ip2c;
                }

                i = blocked_end;
            }
        }

    NPY_PRAGMA_NOVECTOR
        for(; i < n; i++) {
            op1[i] = ip1[i] / ip2c;
        }
    } 
    else if (IS_BINARY_REDUCE) {
        BINARY_REDUCE_LOOP(@type@) {
            io1 /= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
    } 
    else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 / in2;
        }
    }
}

/**begin repeat1
 * #kind = fmax, fmin#
 * #VML = Fmax, Fmin#
 * #OP = >=, <=#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_BINARY_CONT(@type@, @type@);
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same1 && disjoint_or_same2;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, @type@, args[0], args[1], args[2]);
        /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
    }
    else {
        if (IS_BINARY_REDUCE) {
            BINARY_REDUCE_LOOP(@type@) {
                const @type@ in2 = *(@type@ *)ip2;
                /* Order of operations important for MSVC 2015 */
                io1 = (io1 @OP@ in2 || isnan(in2)) ? io1 : in2;
            }
            *((@type@ *)iop1) = io1;
        }
        else {
            BINARY_LOOP {
                const @type@ in1 = *(@type@ *)ip1;
                const @type@ in2 = *(@type@ *)ip2;
                /* Order of operations important for MSVC 2015 */
                *((@type@ *)op1) = (in1 @OP@ in2 || isnan(in2)) ? in1 : in2;
            }
        }
        feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
    }
}
/**end repeat1**/

/**begin repeat1
 * # kind = copysign, nextafter#
 * # VML = CopySign, NextAfter#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_BINARY_CONT(@type@, @type@);
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same1 && disjoint_or_same2;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, @type@, args[0], args[1], args[2]);
        /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
    }
    else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1)= @kind@@c@(in1, in2);
        }
    }
}
/**end repeat1**/

/**begin repeat1
 * # kind = cos, sin, tan, arccos, arcsin, arctan, cosh, sinh, tanh, arccosh, arcsinh, arctanh, fabs, floor, ceil, rint, trunc, cbrt, sqrt, expm1, log, log2, log1p, log10#
 * # func = cos, sin, tan, acos, asin, atan, cosh, sinh, tanh, acosh, asinh, atanh, fabs, floor, ceil, rint, trunc, cbrt, sqrt, expm1, log, log2, log1p, log10#
 * # VML = Cos, Sin, Tan, Acos, Asin, Atan, Cosh, Sinh, Tanh, Acosh, Asinh, Atanh, Abs, Floor, Ceil, Rint, Trunc, Cbrt, Sqrt, Expm1, Ln, Log2, Log1p, Log10#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@s@@VML@, dimensions[0], @type@, @type@, @type@, args[0], args[1]);
        /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    }
    else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @func@@c@(in1);
        )
    }
}
/**end repeat1**/

/**begin repeat1
 * # kind = exp, exp2#
 * # VML = Exp, Exp2#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;
    int ignore_fpstatus = 0;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        ignore_fpstatus = 1;
        CHUNKED_VML_CALL2(v@s@@VML@, dimensions[0], @type@, @type@, @type@, args[0], args[1]);
        /* v@s@Exp(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } 
    else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            const int invalid_cases = npy_isnan(in1) || in1 == NPY_INFINITY || in1 == -NPY_INFINITY;
            ignore_fpstatus |= invalid_cases;
            *(@type@ *)op1 = @kind@@c@(in1);
        )
    }
    if (ignore_fpstatus) {
        feclearexcept(FE_OVERFLOW | FE_UNDERFLOW | FE_INVALID);
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_absolute(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL2(v@s@Abs, dimensions[0], @type@, @type@, @type@, args[0], args[1]);
        /* v@s@Abs(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    }
    else {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ tmp = in1 > 0 ? in1 : -in1;
            /* add 0 to clear -0.0 */
            *((@type@ *)op1) = tmp + 0;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}

void
mkl_umath_@TYPE@_reciprocal(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL2(v@s@Inv, dimensions[0], @type@, @type@, @type@, args[0], args[1]);
        /* v@s@Inv(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    }
    else {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = 1/in1;
        }
    }
}

void
mkl_umath_@TYPE@_square(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL2(v@s@Sqr, dimensions[0], @type@, @type@, @type@, args[0], args[1]);
        /* v@s@Sqr(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    }
    else {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = in1*in1;
        }
    }
}

/* TODO: USE MKL */
void
mkl_umath_@TYPE@_modf(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = modf@c@(in1, (@type@ *)op2);
    }
}

/**begin repeat1
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal,
 *        logical_and, logical_or#
 * #OP = ==, !=, <, <=, >, >=, &&, ||#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((npy_bool *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_logical_xor(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const int t1 = !!*(@type@ *)ip1;
        const int t2 = !!*(@type@ *)ip2;
        *((npy_bool *)op1) = (t1 != t2);
    }
}

void
mkl_umath_@TYPE@_logical_not(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((npy_bool *)op1) = !in1;
    }
}

/**begin repeat1
 * #kind = isnan, isinf, isfinite, signbit#
 * #func = isnan, isinf, isfinite, signbit#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((npy_bool *)op1) = @func@(in1) != 0;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

void
mkl_umath_@TYPE@_conjugate(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = in1;
    }
}

void
mkl_umath_@TYPE@_spacing(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = (spacing@c@(in1));
    }
}

void
mkl_umath_@TYPE@_negative(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = -in1;
        }
    }
}

void
mkl_umath_@TYPE@_positive(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = +in1;
    }
}

void
mkl_umath_@TYPE@_sign(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /* Sign of nan is nan */
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = in1 > 0 ? 1 : (in1 < 0 ? -1 : (in1 == 0 ? 0 : in1));
    }
}

void
mkl_umath_@TYPE@_divmod(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        const @type@ in2 = *(@type@ *)ip2;
        *((@type@ *)op1) = divmod@c@(in1, in2, (@type@ *)op2);
    }
}

void
mkl_umath_@TYPE@_frexp(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = frexp@c@(in1, (int *)op2);
    }
}

void
mkl_umath_@TYPE@_ldexp(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const int in2 = *(int *)ip2;
        *((@type@ *)op1) = ldexp@c@(in1, in2);
    }
}

#ifdef USE_NUMPY_2
void
mkl_umath_@TYPE@_ldexp_int64(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /*
     * Additional loop to handle npy_long integer inputs (cf. #866, #1633).
     * npy_long != npy_int on many 64-bit platforms, so we need this second loop
     * to handle the default (and larger) integer types.
     */
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const npy_int64 in2 = *(npy_int64 *)ip2;
        if (((int)in2) == in2) {
            /* Range OK */
            *((@type@ *)op1) = ldexp@c@(in1, ((int)in2));
        }
        else {
            /*
             * Outside npy_int range -- also ldexp will overflow in this case,
             * given that exponent has less bits than npy_int.
             */
            if (in2 > 0) {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MAX_INT);
            }
            else {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MIN_INT);
            }
        }
    }
}
#else
void
mkl_umath_@TYPE@_ldexp_long(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /*
     * Additional loop to handle npy_long integer inputs (cf. #866, #1633).
     * npy_long != npy_int on many 64-bit platforms, so we need this second loop
     * to handle the default integer type.
     */
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const long in2 = *(long *)ip2;
        if (((int)in2) == in2) {
            /* Range OK */
            *((@type@ *)op1) = ldexp@c@(in1, ((int)in2));
        }
        else {
            /*
             * Outside npy_int range -- also ldexp will overflow in this case,
             * given that exponent has less bits than npy_int.
             */
            if (in2 > 0) {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MAX_INT);
            }
            else {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MIN_INT);
            }
        }
    }
}
#endif
/**end repeat**/

/*
 *****************************************************************************
 **                           COMPLEX LOOPS                                 **
 *****************************************************************************
 */
/* TODO: USE MKL for pow, exp, ln, log10, sqrt, trigonometric functions and hyperbolic functions */

#define CGE(xr,xi,yr,yi) ((xr > yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi >= yi))
#define CLE(xr,xi,yr,yi) ((xr < yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi <= yi))
#define CGT(xr,xi,yr,yi) ((xr > yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi > yi))
#define CLT(xr,xi,yr,yi) ((xr < yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi < yi))
#define CEQ(xr,xi,yr,yi) (xr == yr && xi == yi)
#define CNE(xr,xi,yr,yi) (xr != yr || xi != yi)

/**begin repeat
 * complex types
 * #TYPE = CFLOAT, CDOUBLE#
 * #ftype = npy_float, npy_double#
 * #type = npy_cfloat, npy_cdouble#
 * #mkl_type = MKL_Complex8, MKL_Complex16#
 * #mkl_op_type = float, double#
 * #c = f, #
 * #C = F, # 
 * #s = c, z#
 */

/* similar to pairwise sum of real floats */
#if defined(__ICC) || defined(__INTEL_COMPILER)
#ifdef _MSC_VER
#pragma intel optimization_level 1
#else
#pragma intel optimization_level 2
#endif
#endif
static void
pairwise_sum_@TYPE@(@ftype@ *rr, @ftype@ * ri, char * a, npy_intp n, npy_intp stride)
{
    assert(n % 2 == 0);
    if (n < 8) {
        npy_intp i;

        *rr = -0.0;
        *ri = -0.0;
        for (i = 0; i < n; i += 2) {
            *rr += *((@ftype@ *)(a + i * stride + 0));
            *ri += *((@ftype@ *)(a + i * stride + sizeof(@ftype@)));
        }
        return;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        @ftype@ r[8];

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = *((@ftype@ *)(a + 0 * stride));
        r[1] = *((@ftype@ *)(a + 0 * stride + sizeof(@ftype@)));
        r[2] = *((@ftype@ *)(a + 2 * stride));
        r[3] = *((@ftype@ *)(a + 2 * stride + sizeof(@ftype@)));
        r[4] = *((@ftype@ *)(a + 4 * stride));
        r[5] = *((@ftype@ *)(a + 4 * stride + sizeof(@ftype@)));
        r[6] = *((@ftype@ *)(a + 6 * stride));
        r[7] = *((@ftype@ *)(a + 6 * stride + sizeof(@ftype@)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512 /(npy_intp)sizeof(@ftype@))*stride, 0, 3);
        r[0] += *((@ftype@ *)(a + (i + 0) * stride));
            r[1] += *((@ftype@ *)(a + (i + 0) * stride + sizeof(@ftype@)));
        r[2] += *((@ftype@ *)(a + (i + 2) * stride));
            r[3] += *((@ftype@ *)(a + (i + 2) * stride + sizeof(@ftype@)));
        r[4] += *((@ftype@ *)(a + (i + 4) * stride));
            r[5] += *((@ftype@ *)(a + (i + 4) * stride + sizeof(@ftype@)));
        r[6] += *((@ftype@ *)(a + (i + 6) * stride));
            r[7] += *((@ftype@ *)(a + (i + 6) * stride + sizeof(@ftype@)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        *rr = ((r[0] + r[2]) + (r[4] + r[6]));
        *ri = ((r[1] + r[3]) + (r[5] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i+=2) {
            *rr += *((@ftype@ *)(a + i * stride + 0));
            *ri += *((@ftype@ *)(a + i * stride + sizeof(@ftype@)));
        }
        return;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        @ftype@ rr1, ri1, rr2, ri2;
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        pairwise_sum_@TYPE@(&rr1, &ri1, a, n2, stride);
        pairwise_sum_@TYPE@(&rr2, &ri2, a + n2 * stride, n - n2, stride);
        *rr = rr1 + rr2;
        *ri = ri1 + ri2;
        return;
    }
}

/**begin repeat1
 * #kind = add, subtract#
 * #OP = +, -#
 * #PW = 1, 0#
 * #VML = Add, Sub# 
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_BINARY_CONT(@mkl_type@, @mkl_type@);
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same1 && disjoint_or_same2;

    if (can_vectorize && dimensions[0] > VML_ASM_THRESHOLD) {
        CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, @mkl_type@, args[0], args[1], args[2]);
        /* v@s@@VML@(dimensions[0], (@mkl_type@*) args[0], (@mkl_type@*) args[1], (@mkl_type@*) args[2]); */
    }
    else { 
        if (IS_BINARY_REDUCE && @PW@) {
            npy_intp n = dimensions[0];
            @ftype@ * or = ((@ftype@ *)args[0]);
            @ftype@ * oi = ((@ftype@ *)args[0]) + 1;
            @ftype@ rr, ri;

            pairwise_sum_@TYPE@(&rr, &ri, args[1], n * 2, steps[1] / 2);
            *or @OP@= rr;
            *oi @OP@= ri;
            return;
        } 
        else {
            BINARY_LOOP {
                const @ftype@ in1r = ((@ftype@ *)ip1)[0];
                const @ftype@ in1i = ((@ftype@ *)ip1)[1];
                const @ftype@ in2r = ((@ftype@ *)ip2)[0];
                const @ftype@ in2i = ((@ftype@ *)ip2)[1];
                ((@ftype@ *)op1)[0] = in1r @OP@ in2r;
                ((@ftype@ *)op1)[1] = in1i @OP@ in2i;
            }
        }
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_multiply(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_BINARY_CONT(@mkl_type@, @mkl_type@);
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same1 && disjoint_or_same2;

    if (can_vectorize && dimensions[0] > VML_ASM_THRESHOLD) {
        CHUNKED_VML_CALL3(v@s@Mul, dimensions[0], @type@, @mkl_type@, args[0], args[1], args[2]);
        /* v@s@Mul(dimensions[0], (@mkl_type@*) args[0], (@mkl_type@*) args[1], (@mkl_type@*) args[2]); */
    }
    else {
        BINARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            const @ftype@ in2r = ((@ftype@ *)ip2)[0];
            const @ftype@ in2i = ((@ftype@ *)ip2)[1];
            ((@ftype@ *)op1)[0] = in1r*in2r - in1i*in2i;
            ((@ftype@ *)op1)[1] = in1r*in2i + in1i*in2r;
        }
    }
}

void
mkl_umath_@TYPE@_divide(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_BINARY_CONT(@mkl_type@, @mkl_type@);
    const int disjoint_or_same1 = DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@));
    const int disjoint_or_same2 = DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same1 && disjoint_or_same2;    

    if (can_vectorize && dimensions[0] > VML_D_THRESHOLD) {
        CHUNKED_VML_CALL3(v@s@Div, dimensions[0], @type@, @mkl_type@, args[0], args[1], args[2]);
        /* v@s@Div(dimensions[0], (@mkl_type@*) args[0], (@mkl_type@*) args[1], (@mkl_type@*) args[2]); */
    }
    else {       
        BINARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            const @ftype@ in2r = ((@ftype@ *)ip2)[0];
            const @ftype@ in2i = ((@ftype@ *)ip2)[1];
            const @ftype@ in2r_abs = fabs@c@(in2r);
            const @ftype@ in2i_abs = fabs@c@(in2i);
            if (in2r_abs >= in2i_abs) {
                if (in2r_abs == 0 && in2i_abs == 0) {
                    /* divide by zero should yield a complex inf or nan */
                    ((@ftype@ *)op1)[0] = in1r/in2r_abs;
                    ((@ftype@ *)op1)[1] = in1i/in2i_abs;
                }
                else {
                    const @ftype@ rat = in2i/in2r;
                    const @ftype@ scl = 1.0@c@/(in2r + in2i*rat);
                    ((@ftype@ *)op1)[0] = (in1r + in1i*rat)*scl;
                    ((@ftype@ *)op1)[1] = (in1i - in1r*rat)*scl;
                }
            }
            else {
                const @ftype@ rat = in2r/in2i;
                const @ftype@ scl = 1.0@c@/(in2i + in2r*rat);
                ((@ftype@ *)op1)[0] = (in1r*rat + in1i)*scl;
                ((@ftype@ *)op1)[1] = (in1i*rat - in1r)*scl;
            }
        }
    }
}

/**begin repeat1
 * #kind= greater, greater_equal, less, less_equal, equal, not_equal#
 * #OP = CGT, CGE, CLT, CLE, CEQ, CNE#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = @OP@(in1r,in1i,in2r,in2i);
    }
}
/**end repeat1**/

/**begin repeat1
   #kind = logical_and, logical_or#
   #OP1 = ||, ||#
   #OP2 = &&, ||#
*/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = (in1r @OP1@ in1i) @OP2@ (in2r @OP1@ in2i);
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_logical_xor(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        const npy_bool tmp1 = (in1r || in1i);
        const npy_bool tmp2 = (in2r || in2i);
        *((npy_bool *)op1) = tmp1 != tmp2;
    }
}

void
mkl_umath_@TYPE@_logical_not(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        *((npy_bool *)op1) = !(in1r || in1i);
    }
}

/**begin repeat1
 * #kind = isnan, isinf, isfinite#
 * #func = isnan, isinf, isfinite#
 * #OP = ||, ||, &&#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        *((npy_bool *)op1) = @func@(in1r) @OP@ @func@(in1i);
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

void
mkl_umath_@TYPE@_square(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        ((@ftype@ *)op1)[0] = in1r*in1r - in1i*in1i;
        ((@ftype@ *)op1)[1] = in1r*in1i + in1i*in1r;
    }
}

void
mkl_umath_@TYPE@_reciprocal(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        if (fabs@c@(in1i) <= fabs@c@(in1r)) {
            const @ftype@ r = in1i/in1r;
            const @ftype@ d = in1r + in1i*r;
            ((@ftype@ *)op1)[0] = 1/d;
            ((@ftype@ *)op1)[1] = -r/d;
        } 
        else {
            const @ftype@ r = in1r/in1i;
            const @ftype@ d = in1r*r + in1i;
            ((@ftype@ *)op1)[0] = r/d;
            ((@ftype@ *)op1)[1] = -1/d;
        }
    }
}

void
mkl_umath_@TYPE@_conjugate(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func)) {
    const int contig = IS_UNARY_CONT(@mkl_type@, @mkl_type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL2(v@s@Conj, dimensions[0], @type@, @mkl_type@, @mkl_type@, args[0], args[1]);
        /* v@s@Conj(dimensions[0], (@mkl_type@*) args[0], (@mkl_type@*) args[1]); */
    } 
    else {
        UNARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            ((@ftype@ *)op1)[0] = in1r;
            ((@ftype@ *)op1)[1] = -in1i;
        }
    }
}

void
mkl_umath_@TYPE@_absolute(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@mkl_type@, @mkl_op_type@);
    const int disjoint_or_same = DISJOINT_OR_SAME_TWO_DTYPES(args[0], args[1], dimensions[0], sizeof(@type@), sizeof(@ftype@));
    const int can_vectorize = contig && disjoint_or_same;
    int ignore_fpstatus = 0;
    
    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        ignore_fpstatus = 1;
        CHUNKED_VML_CALL2(v@s@Abs, dimensions[0], @type@, @mkl_type@, @mkl_op_type@, args[0], args[1]);
        /* v@s@Abs(dimensions[0], (@mkl_type@*) args[0], (@mkl_op_type@*) args[1]); */
    } 
    else {
        UNARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            *((@ftype@ *)op1) = hypot@c@(in1r, in1i);
        }
    }
    if (ignore_fpstatus) {
        feclearexcept(FE_INVALID);
    }
}

void
mkl_umath_@TYPE@_sign(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    #ifdef USE_NUMPY_2
        UNARY_LOOP {
            const @ftype@ in_r = ((@ftype@ *)ip1)[0];
            const @ftype@ in_i = ((@ftype@ *)ip1)[1];
            const @ftype@ in_abs = hypot@c@(in_r, in_i);
            if (NPY_UNLIKELY(npy_isnan(in_abs))) {
                ((@ftype@ *)op1)[0] = NPY_NAN@C@;
                ((@ftype@ *)op1)[1] = NPY_NAN@C@;
            }
            else if (NPY_UNLIKELY(npy_isinf(in_abs))) {
                if (npy_isinf(in_r)) {
                    if (npy_isinf(in_i)) {
                        ((@ftype@ *)op1)[0] = NPY_NAN@C@;
                        ((@ftype@ *)op1)[1] = NPY_NAN@C@;
                    }
                    else {
                        ((@ftype@ *)op1)[0] = in_r > 0 ? 1.: -1.;
                        ((@ftype@ *)op1)[1] = 0.;
                    }
                }
                else {
                    ((@ftype@ *)op1)[0] = 0.;
                    ((@ftype@ *)op1)[1] = in_i > 0 ? 1.: -1.;
                }
            }
            else if (NPY_UNLIKELY(in_abs == 0)) {
                ((@ftype@ *)op1)[0] = 0.@C@;
                ((@ftype@ *)op1)[1] = 0.@C@;
            }
            else{
                ((@ftype@ *)op1)[0] = in_r / in_abs;
                ((@ftype@ *)op1)[1] = in_i / in_abs;
            }
        }
    #else
        /* fixme: sign of nan is currently 0 */
        UNARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            ((@ftype@ *)op1)[0] = CGT(in1r, in1i, 0.0, 0.0) ? 1 :
                                (CLT(in1r, in1i, 0.0, 0.0) ? -1 :
                                (CEQ(in1r, in1i, 0.0, 0.0) ? 0 : NPY_NAN@C@));
            ((@ftype@ *)op1)[1] = 0;
        }
        feclearexcept(FE_INVALID);
    #endif
}

/**begin repeat1
 * #kind = fmax, fmin#
 * #OP = CGE, CLE#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        if (isnan(in2r) || isnan(in2i) || @OP@(in1r, in1i, in2r, in2i)) {
            ((@ftype@ *)op1)[0] = in1r;
            ((@ftype@ *)op1)[1] = in1i;
        }
        else {
            ((@ftype@ *)op1)[0] = in2r;
            ((@ftype@ *)op1)[1] = in2i;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

/**end repeat**/

#undef CGE
#undef CLE
#undef CGT
#undef CLT
#undef CEQ
#undef CNE

/*
 *****************************************************************************
 **                              END LOOPS                                  **
 *****************************************************************************
 */
