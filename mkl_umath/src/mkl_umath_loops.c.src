/*
 * Copyright (c) 2019-2023, Intel Corporation
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 *     * Redistributions of source code must retain the above copyright notice,
 *       this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of Intel Corporation nor the names of its contributors
 *       may be used to endorse or promote products derived from this software
 *       without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/* -*- c -*- */
#include "mkl.h"
#include <float.h>
#include <fenv.h>
#include "Python.h"

#define NPY_NO_DEPRECATED_API NPY_API_VERSION

#include "numpy/npy_common.h"
#include "numpy/ndarraytypes.h"
#include "numpy/ndarrayobject.h"
#include "numpy/ufuncobject.h"
#include "numpy/npy_math.h"
#include "blocking_utils.h"
#include "mkl_umath_loops.h"

/* Adapated from NumPy's source code.
 * https://github.com/numpy/numpy/blob/main/LICENSE.txt */

/*
 * largest simd vector size in bytes numpy supports
 * it is currently a extremely large value as it is only used for memory
 * overlap checks
 */
#ifndef NPY_MAX_SIMD_SIZE
#define NPY_MAX_SIMD_SIZE 1024
#endif

/*
 * cutoff blocksize for pairwise summation
 * decreasing it decreases errors slightly as more pairs are summed but
 * also lowers performance, as the inner loop is unrolled eight times it is
 * effectively 16
 */
#define PW_BLOCKSIZE    128
#define VML_TRANSCEDENTAL_THRESHOLD 8192
#define VML_ASM_THRESHOLD 100000
#define VML_D_THRESHOLD 8000

#define MKL_INT_MAX ((npy_intp) ((~((MKL_UINT) 0)) >> 1))

#define CHUNKED_VML_CALL2(vml_func, n, type, in1, op1)   \
    do {                                                 \
        npy_intp _n_ = (n);                              \
        const npy_intp _chunk_size = MKL_INT_MAX;        \
        type *in1p = (type *) (in1);                     \
        type *op1p = (type *) (op1);                     \
        while (_n_ > _chunk_size) {                      \
            vml_func((MKL_INT) _chunk_size, in1p, op1p); \
            _n_ -= _chunk_size;                          \
            in1p += _chunk_size;                         \
            op1p += _chunk_size;                         \
        }                                                \
        if (_n_) {                                       \
            vml_func((MKL_INT) _n_, in1p, op1p);         \
        }                                                \
    } while (0)

#define CHUNKED_VML_CALL3(vml_func, n, type, in1, in2, op1)     \
    do  {                                                       \
        npy_intp _n_ = (n);                                     \
        const npy_intp _chunk_size = MKL_INT_MAX;               \
        type *in1p = (type *) (in1);                            \
        type *in2p = (type *) (in2);                            \
        type *op1p = (type *) (op1);                            \
        while (_n_ > _chunk_size) {                             \
            vml_func((MKL_INT) _chunk_size, in1p, in2p, op1p);  \
            _n_ -= _chunk_size;                                 \
            in1p += _chunk_size;                                \
            in2p += _chunk_size;                                \
            op1p += _chunk_size;                                \
        }                                                       \
        if (_n_) {                                              \
            vml_func((MKL_INT)_n_, in1p, in2p, op1p);           \
        }                                                       \
    } while(0)


#define CHUNKED_VML_LINEARFRAC_CALL(vml_func, n, type, in1, op1, scaleA, shiftA, scaleB, shiftB) \
     do {                                                                                        \
        npy_intp _n_ = (n);                                                                      \
        const npy_intp _chunk_size = MKL_INT_MAX;                                                \
        type *in1p = (type *) (in1);                                                             \
        type *op1p = (type *) (op1);                                                             \
        const type _scaleA = (scaleA);                                                           \
        const type _shiftA = (shiftA);                                                           \
        const type _scaleB = (scaleB);                                                           \
        const type _shiftB = (shiftB);                                                           \
        while (_n_ > _chunk_size) {                                                              \
            vml_func(_chunk_size, in1p, in1p, _scaleA, _shiftA, _scaleB, _shiftB, op1p);         \
            _n_ -= _chunk_size;                                                                  \
            in1p += _chunk_size;                                                                 \
            op1p += _chunk_size;                                                                 \
        }                                                                                        \
        if (_n_) {                                                                               \
            vml_func((MKL_INT)_n_, in1p, in1p, _scaleA, _shiftA, _scaleB, _shiftB, op1p);        \
        }                                                                                        \
    } while(0)

/* for pointers p1, and p2 pointing at contiguous arrays n-elements of size s, are arrays disjoint or same
 *  when these conditions are not met VML functions may product incorrect output
 */
#define DISJOINT_OR_SAME(p1, p2, n, s) (((p1) == (p2)) || ((p2) + (n)*(s) < (p1)) || ((p1) + (n)*(s) < (p2)) )

/*
 * include vectorized functions and dispatchers
 * this file is safe to include also for generic builds
 * platform specific instructions are either masked via the proprocessor or
 * runtime detected
 */

/** Provides the various *_LOOP macros */
#include "fast_loop_macros.h"
#include <stdio.h>

static inline npy_double spacing(npy_double x) {
    if (isinf(x))
    return ((npy_double) NAN);
    return copysign(nextafter(fabs(x), ((npy_double) INFINITY)), x) - x;
}

static inline npy_float spacingf(npy_float x) {
    if (isinff(x))
    return ((npy_float) NAN);

    return copysignf(nextafterf(fabsf(x), INFINITY), x) - x;
}


/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #c = f, #
 */
/*
 * Python version of divmod.
 *
 * The implementation is mostly copied from cpython 3.5.
 */
static inline @type@
divmod@c@(@type@ a, @type@ b, @type@ *modulus)
{
    @type@ div, mod, floordiv;

    mod = fmod@c@(a, b);

    if (!b) {
        /* If b == 0, return result of fmod. For IEEE is nan */
        *modulus = mod;
        return mod;
    }

    /* a - mod should be very nearly an integer multiple of b */
    div = (a - mod) / b;

    /* adjust fmod result to conform to Python convention of remainder */
    if (mod) {
        if ((b < 0) != (mod < 0)) {
            mod += b;
            div -= 1.0@c@;
        }
    }
    else {
        /* if mod is zero ensure correct sign */
        mod = copysign@c@(0, b);
    }

    /* snap quotient to nearest integral value */
    if (div) {
        floordiv = floor@c@(div);
        if (div - floordiv > 0.5@c@)
            floordiv += 1.0@c@;
    }
    else {
        /* if div is zero ensure correct sign */
        floordiv = copysign@c@(0, a/b);
    }

    *modulus = mod;
    return floordiv;
}
/**end repeat**/

/*
 *****************************************************************************
 **                             FLOAT LOOPS                                 **
 *****************************************************************************
 */

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = sqrtf, sqrt#
 */

void
mkl_umath_@TYPE@_sqrt(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if(can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Sqrt, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Sqrt(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = (1.0f)/sqrtf, (1.0)/sqrt#
 */

void
mkl_umath_@TYPE@_invsqrt(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if(can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@InvSqrt, dimensions[0], @type@, args[0], args[1]);
        /* v@c@InvSqrt(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #A = F, #
 *  #c = s, d#
 *  #scalarf = expf, exp#
 */

void
mkl_umath_@TYPE@_exp(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;
    int ignore_fpstatus = 0;

    if(can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        ignore_fpstatus = 1;
        CHUNKED_VML_CALL2(v@c@Exp, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Exp(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            ignore_fpstatus |= ((in1 == -NPY_INFINITY@A@) ? 1 : 0);
            *(@type@ *)op1 = @scalarf@(in1);
    )
    }
    if(ignore_fpstatus) {
        feclearexcept(FE_DIVBYZERO | FE_OVERFLOW | FE_UNDERFLOW | FE_INVALID);
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = exp2f, exp2#
 */

/* TODO: Use VML */
void
mkl_umath_@TYPE@_exp2(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    UNARY_LOOP_DISPATCH(
        @type@, @type@
        ,
        can_vectorize
        ,
        const @type@ in1 = *(@type@ *)ip1;
        *(@type@ *)op1 = @scalarf@(in1);
    )
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = expm1f, expm1#
 */

void
mkl_umath_@TYPE@_expm1(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if(can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
        CHUNKED_VML_CALL2(v@c@Expm1, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Expm1(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = erff, erf#
 */

void
mkl_umath_@TYPE@_erf(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Erf, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Erf(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = logf, log#
 */

void
mkl_umath_@TYPE@_log(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Ln, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Ln(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = log2f, log2#
 */

/* TODO: Use VML */
void
mkl_umath_@TYPE@_log2(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    UNARY_LOOP_DISPATCH(
        @type@, @type@
        ,
        can_vectorize
        ,
        const @type@ in1 = *(@type@ *)ip1;
        *(@type@ *)op1 = @scalarf@(in1);
    )
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = log10f, log10#
 */

void
mkl_umath_@TYPE@_log10(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Log10, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Log10(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = log1pf, log1p#
 */

void
mkl_umath_@TYPE@_log1p(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Log1p, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Log1p(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = cosf, cos#
 */

void
mkl_umath_@TYPE@_cos(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Cos, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Cos(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = sinf, sin#
 */

void
mkl_umath_@TYPE@_sin(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Sin, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Sin(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = tanf, tan#
 */

void
mkl_umath_@TYPE@_tan(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Tan, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Tan(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = acosf, acos#
 */

void
mkl_umath_@TYPE@_arccos(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Acos, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Acos(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = asinf, asin#
 */

void
mkl_umath_@TYPE@_arcsin(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Asin, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Asin(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = atanf, atan#
 */

void
mkl_umath_@TYPE@_arctan(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Atan, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Atan(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = coshf, cosh#
 */

void
mkl_umath_@TYPE@_cosh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Cosh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Cosh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = sinhf, sinh#
 */

void
mkl_umath_@TYPE@_sinh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Sinh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Sinh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = tanhf, tanh#
 */

void
mkl_umath_@TYPE@_tanh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Tanh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Tanh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = acoshf, acosh#
 */

void
mkl_umath_@TYPE@_arccosh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Acosh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Acosh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = asinhf, asinh#
 */

void
mkl_umath_@TYPE@_arcsinh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Asinh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Asinh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = atanhf, atanh#
 */

void
mkl_umath_@TYPE@_arctanh(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Atanh, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Atanh(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = fabsf, fabs#
 */

void
mkl_umath_@TYPE@_fabs(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    UNARY_LOOP_DISPATCH(
        @type@, @type@
        ,
        can_vectorize
        ,
        const @type@ in1 = *(@type@ *)ip1;
        *(@type@ *)op1 = @scalarf@(in1);
    )
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = floorf, floor#
 */

void
mkl_umath_@TYPE@_floor(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Floor, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Floor(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = ceilf, ceil#
 */

void
mkl_umath_@TYPE@_ceil(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Ceil, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Ceil(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = rintf, rint#
 */

void
mkl_umath_@TYPE@_rint(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Rint, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Rint(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = truncf, trunc#
 */

void
mkl_umath_@TYPE@_trunc(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if( can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Trunc, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Trunc(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = s, d#
 *  #scalarf = cbrtf, cbrt#
 */

void
mkl_umath_@TYPE@_cbrt(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    const int contig = IS_UNARY_CONT(@type@, @type@);
    const int disjoint_or_same = DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@));
    const int can_vectorize = contig && disjoint_or_same;

    if (can_vectorize && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD)
    {
        CHUNKED_VML_CALL2(v@c@Cbrt, dimensions[0], @type@, args[0], args[1]);
        /* v@c@Cbrt(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else {
        UNARY_LOOP_DISPATCH(
            @type@, @type@
            ,
            can_vectorize
            ,
            const @type@ in1 = *(@type@ *)ip1;
            *(@type@ *)op1 = @scalarf@(in1);
        )
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #dtype = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = f, #
 *  #C = F, #
 *  #trf = , #
 */

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
#if defined(__ICC) || defined(__INTEL_COMPILER)
#ifdef _MSC_VER
#pragma intel optimization_level 1
#else
#pragma intel optimization_level 2
#endif
#endif
static @type@
pairwise_sum_@TYPE@(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        @type@ res = 0.;

        for (i = 0; i < n; i++) {
            res += @trf@(*((@dtype@*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        @type@ r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = @trf@(*((@dtype@ *)(a + 0 * stride)));
        r[1] = @trf@(*((@dtype@ *)(a + 1 * stride)));
        r[2] = @trf@(*((@dtype@ *)(a + 2 * stride)));
        r[3] = @trf@(*((@dtype@ *)(a + 3 * stride)));
        r[4] = @trf@(*((@dtype@ *)(a + 4 * stride)));
        r[5] = @trf@(*((@dtype@ *)(a + 5 * stride)));
        r[6] = @trf@(*((@dtype@ *)(a + 6 * stride)));
        r[7] = @trf@(*((@dtype@ *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(@dtype@))*stride, 0, 3);
            r[0] += @trf@(*((@dtype@ *)(a + (i + 0) * stride)));
            r[1] += @trf@(*((@dtype@ *)(a + (i + 1) * stride)));
            r[2] += @trf@(*((@dtype@ *)(a + (i + 2) * stride)));
            r[3] += @trf@(*((@dtype@ *)(a + (i + 3) * stride)));
            r[4] += @trf@(*((@dtype@ *)(a + (i + 4) * stride)));
            r[5] += @trf@(*((@dtype@ *)(a + (i + 5) * stride)));
            r[6] += @trf@(*((@dtype@ *)(a + (i + 6) * stride)));
            r[7] += @trf@(*((@dtype@ *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += @trf@(*((@dtype@ *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return pairwise_sum_@TYPE@(a, n2, stride) +
               pairwise_sum_@TYPE@(a + n2 * stride, n - n2, stride);
    }
}

/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = f, #
 *  #C = F, #
 *  #s = s, d#
 *  #SUPPORTED_BY_VML = 1, 1#
 */

/**begin repeat1
 * Arithmetic
 * # kind = add#
 * # OP = +#
 * # PW = 1#
 * # VML = Add#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_CONT(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
                DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) &&
                DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, args[0], args[1], args[2]);
            /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;

                    if( DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
                        DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1) ) {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        NPY_PRAGMA_VECTOR
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
                        }
                    } else {
                        NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                        for(j = 0; j < j_max; j++) {
                            op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
                        }
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S1(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[1], args[2], 1.0, *(@type@*)args[0], 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[1], (@type@*) args[1], 1.0, *(@type@*)args[0], 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip1c = ip1[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip2_aligned = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

                    NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1c @OP@ ip2_aligned[j];
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S2(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[0], args[2], 1.0, *(@type@*)args[1], 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[0], (@type@*) args[0], 1.0, *(@type@*)args[1], 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip2c = ip2[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }

            if (blocked_end > peel) {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip1_aligned = ip1 + peel;

                    NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1_aligned[j] @OP@ ip2c;
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }
        }
    } else if (IS_BINARY_REDUCE) {
#if @PW@
        @type@ * iop1 = (@type@ *)args[0];
        npy_intp n = dimensions[0];

        *iop1 @OP@= pairwise_sum_@TYPE@(args[1], n, steps[1]);
#else
        BINARY_REDUCE_LOOP(@type@) {
            io1 @OP@= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
#endif
    } else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

/**begin repeat1
 * Arithmetic
 * # kind = subtract#
 * # OP = -#
 * # PW = 0#
 * # VML = Sub#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_CONT(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, args[0], args[1], args[2]);
            /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

            if( DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
            DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1) ) {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            NPY_PRAGMA_VECTOR
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            } else {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S1(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@))) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[1], args[2], -1.0, *(@type@*)args[0], 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[1], (@type@*) args[1], -1.0, *(@type@*)args[0], 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip1c = ip1[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip2_aligned = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

                    NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1c @OP@ ip2_aligned[j];
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S2(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[0], args[2], 1.0, -(*(@type@*)args[1]), 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[0], (@type@*) args[0], 1.0, -*(@type@*)args[1], 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip2c = ip2[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip1_aligned = ip1 + peel;

                    NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1_aligned[j] @OP@ ip2c;
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }
        }
    } else if (IS_BINARY_REDUCE) {
#if @PW@
        @type@ * iop1 = (@type@ *)args[0];
        npy_intp n = dimensions[0];

        *iop1 @OP@= pairwise_sum_@TYPE@(args[1], n, steps[1]);
#else
        BINARY_REDUCE_LOOP(@type@) {
            io1 @OP@= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
#endif
    } else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

/**begin repeat1
 * Arithmetic
 * # kind = multiply#
 * # OP = *#
 * # PW = 0#
 * # VML = Mul#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_CONT(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, args[0], args[1], args[2]);
            /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel;
                    @type@ *ip2_shifted = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

            if( DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
            DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1) ) {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            NPY_PRAGMA_VECTOR
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            } else {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S1(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[1], args[2], *(@type@*)args[0], 0.0, 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[1], (@type@*) args[1], *(@type@*)args[0], 0.0, 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip1c = ip1[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *ip2_aligned = ip2 + peel;
                    @type@ *op1_shifted = op1 + peel;

                    NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1c @OP@ ip2_aligned[j];
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1c @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S2(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_ASM_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_LINEARFRAC_CALL(v@s@LinearFrac, dimensions[0], @type@, args[0], args[2], *(@type@*)args[1], 0.0, 0.0, 1.0);
            /* v@s@LinearFrac(dimensions[0], (@type@*) args[0], (@type@*) args[0], *(@type@*)args[1], 0.0, 0.0, 1.0, (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

            const @type@ ip2c = ip2[0];
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }

            {
                npy_intp j, j_max = blocked_end - peel;
                if (j_max > 0) {
                    @type@ *op1_shifted = op1 + peel;
                    @type@ *ip1_aligned = ip1 + peel;

                    NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                    for(j = 0; j < j_max; j++) {
                        op1_shifted[j] = ip1_aligned[j] @OP@ ip2c;
                    }

                    i = blocked_end;
                }
            }

            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2c;
            }
        }
    } else if (IS_BINARY_REDUCE) {
#if @PW@
        @type@ * iop1 = (@type@ *)args[0];
        npy_intp n = dimensions[0];

        *iop1 @OP@= pairwise_sum_@TYPE@(args[1], n, steps[1]);
#else
        BINARY_REDUCE_LOOP(@type@) {
            io1 @OP@= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
#endif
    } else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

/**begin repeat1
 * Arithmetic
 * # kind = divide#
 * # OP = /#
 * # PW = 0#
 * # VML = Div#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_CONT(@type@, @type@)) {
#if @SUPPORTED_BY_VML@
        if(dimensions[0] > VML_D_THRESHOLD &&
               DISJOINT_OR_SAME(args[0], args[2], dimensions[0], sizeof(@type@)) &&
               DISJOINT_OR_SAME(args[1], args[2], dimensions[0], sizeof(@type@)) ) {
            CHUNKED_VML_CALL3(v@s@@VML@, dimensions[0], @type@, args[0], args[1], args[2]);
            /* v@s@@VML@(dimensions[0], (@type@*) args[0], (@type@*) args[1], (@type@*) args[2]); */
        } else
#endif
        {
            @type@ *ip1 = (@type@*)args[0];
            @type@ *ip2 = (@type@*)args[1];
            @type@ *op1 = (@type@*)args[2];
            const npy_intp vsize = 64;
            const npy_intp n = dimensions[0];
            const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
            const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
            npy_intp i;

        NPY_PRAGMA_NOVECTOR
            for(i = 0; i < peel; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }

            {
                npy_intp j, j_max = blocked_end - peel;
        j_max &= (~0xf);
        const npy_intp blocked_end = j_max + peel;
                if (j_max > 0) {
                    @type@ *ip1_aligned = ip1 + peel, *op1_shifted = op1 + peel, *ip2_shifted = ip2 + peel;

            if( DISJOINT_OR_SAME(op1_shifted, ip1_aligned, j_max, 1) &&
            DISJOINT_OR_SAME(op1_shifted, ip2_shifted, j_max, 1) ) {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            NPY_PRAGMA_VECTOR
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            } else {
            NPY_ASSUME_ALIGNED(ip1_aligned, 64)
            for(j = 0; j < j_max; j++) {
                op1_shifted[j] = ip1_aligned[j] @OP@ ip2_shifted[j];
            }
            }

                    i = blocked_end;
                }
            }

        NPY_PRAGMA_NOVECTOR
            for(; i < n; i++) {
                op1[i] = ip1[i] @OP@ ip2[i];
            }
        }
    } else if(IS_BINARY_CONT_S1(@type@, @type@)) {
        @type@ *ip1 = (@type@*)args[0];
        @type@ *ip2 = (@type@*)args[1];
        @type@ *op1 = (@type@*)args[2];
        const npy_intp vsize = 64;
        const npy_intp n = dimensions[0];
        const npy_intp peel = npy_aligned_block_offset(ip2, sizeof(@type@), vsize, n);
        const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
        npy_intp i;

        const @type@ ip1c = ip1[0];
    NPY_PRAGMA_NOVECTOR
        for(i = 0; i < peel; i++) {
            op1[i] = ip1c @OP@ ip2[i];
        }

        {
            npy_intp j, j_max = blocked_end - peel;
            if (j_max > 0) {
                @type@ *ip2_aligned = ip2 + peel, *op1_shifted = op1 + peel;

                NPY_ASSUME_ALIGNED(ip2_aligned, 64)
                for(j = 0; j < j_max; j++) {
                    op1_shifted[j] = ip1c @OP@ ip2_aligned[j];
                }

                i = blocked_end;
            }
        }

    NPY_PRAGMA_NOVECTOR
        for(; i < n; i++) {
            op1[i] = ip1c @OP@ ip2[i];
        }
    } else if(IS_BINARY_CONT_S2(@type@, @type@)) {
        @type@ *ip1 = (@type@*)args[0];
        @type@ *ip2 = (@type@*)args[1];
        @type@ *op1 = (@type@*)args[2];
        const npy_intp vsize = 64;
        const npy_intp n = dimensions[0];
        const npy_intp peel = npy_aligned_block_offset(ip1, sizeof(@type@), vsize, n);
        const npy_intp blocked_end = npy_blocked_end(peel, sizeof(@type@), vsize, n);
        npy_intp i;

        const @type@ ip2c = ip2[0];
    NPY_PRAGMA_NOVECTOR
        for(i = 0; i < peel; i++) {
            op1[i] = ip1[i] @OP@ ip2c;
        }

        {
            npy_intp j, j_max = blocked_end - peel;
            if (j_max > 0) {
                @type@ *ip1_aligned = ip1 + peel, *op1_shifted = op1 + peel;

                NPY_ASSUME_ALIGNED(ip1_aligned, 64)
                for(j = 0; j < j_max; j++) {
                    op1_shifted[j] = ip1_aligned[j] @OP@ ip2c;
                }

                i = blocked_end;
            }
        }

    NPY_PRAGMA_NOVECTOR
        for(; i < n; i++) {
            op1[i] = ip1[i] @OP@ ip2c;
        }
    } else if (IS_BINARY_REDUCE) {
#if @PW@
        @type@ * iop1 = (@type@ *)args[0];
        npy_intp n = dimensions[0];

        *iop1 @OP@= pairwise_sum_@TYPE@(args[1], n, steps[1]);
#else
        BINARY_REDUCE_LOOP(@type@) {
            io1 @OP@= *(@type@ *)ip2;
        }
        *((@type@ *)iop1) = io1;
#endif
    } else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((@type@ *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

/**begin repeat1
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal,
 *        logical_and, logical_or#
 * #OP = ==, !=, <, <=, >, >=, &&, ||#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            *((npy_bool *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_logical_xor(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const int t1 = !!*(@type@ *)ip1;
        const int t2 = !!*(@type@ *)ip2;
        *((npy_bool *)op1) = (t1 != t2);
    }
}

void
mkl_umath_@TYPE@_logical_not(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((npy_bool *)op1) = !in1;
    }
}

/**begin repeat1
 * #kind = isnan, isinf, isfinite, signbit#
 * #func = isnan, isinf, isfinite, signbit#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((npy_bool *)op1) = @func@(in1) != 0;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

void
mkl_umath_@TYPE@_spacing(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = (spacing@c@(in1));
    }
}

void
mkl_umath_@TYPE@_copysign(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const @type@ in2 = *(@type@ *)ip2;
        *((@type@ *)op1)= copysign@c@(in1, in2);
    }
}

void
mkl_umath_@TYPE@_nextafter(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const @type@ in2 = *(@type@ *)ip2;
        *((@type@ *)op1)= nextafter@c@(in1, in2);
    }
}

/**begin repeat1
 * #kind = maximum, minimum#
 * #OP =  >=, <=#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /*  */
    if (IS_BINARY_REDUCE) {
        {
            BINARY_REDUCE_LOOP(@type@) {
                const @type@ in2 = *(@type@ *)ip2;
                /* Order of operations important for MSVC 2015 */
                io1 = (io1 @OP@ in2 || isnan(io1)) ? io1 : in2;
            }
            *((@type@ *)iop1) = io1;
        }
    }
    else {
        BINARY_LOOP {
            @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            /* Order of operations important for MSVC 2015 */
            in1 = (in1 @OP@ in2 || isnan(in1)) ? in1 : in2;
            *((@type@ *)op1) = in1;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

/**begin repeat1
 * #kind = fmax, fmin#
 * #OP =  >=, <=#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /*  */
    if (IS_BINARY_REDUCE) {
        BINARY_REDUCE_LOOP(@type@) {
            const @type@ in2 = *(@type@ *)ip2;
            /* Order of operations important for MSVC 2015 */
            io1 = (io1 @OP@ in2 || isnan(in2)) ? io1 : in2;
        }
        *((@type@ *)iop1) = io1;
    }
    else {
        BINARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ in2 = *(@type@ *)ip2;
            /* Order of operations important for MSVC 2015 */
            *((@type@ *)op1) = (in1 @OP@ in2 || isnan(in2)) ? in1 : in2;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

void
mkl_umath_@TYPE@_remainder(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const @type@ in2 = *(@type@ *)ip2;
        divmod@c@(in1, in2, (@type@ *)op1);
    }
}

void
mkl_umath_@TYPE@_divmod(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        const @type@ in2 = *(@type@ *)ip2;
        *((@type@ *)op1) = divmod@c@(in1, in2, (@type@ *)op2);
    }
}

void
mkl_umath_@TYPE@_square(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
#if @SUPPORTED_BY_VML@
    if(IS_UNARY_CONT(@type@, @type@) &&
           dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD &&
           DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@)) ) {
        CHUNKED_VML_CALL2(v@s@Sqr, dimensions[0], @type@, args[0], args[1]);
        /* v@s@Sqr(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else
#endif
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = in1*in1;
        }
    }
}

void
mkl_umath_@TYPE@_reciprocal(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
#if @SUPPORTED_BY_VML@
    if(IS_UNARY_CONT(@type@, @type@) &&
           dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD &&
           DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@)) ) {
        CHUNKED_VML_CALL2(v@s@Inv, dimensions[0], @type@, args[0], args[1]);
        /* v@s@Inv(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else
#endif
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = 1/in1;
        }
    }
}

void
mkl_umath_@TYPE@__ones_like(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    OUTPUT_LOOP {
        *((@type@ *)op1) = 1;
    }
}

void
mkl_umath_@TYPE@_conjugate(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = in1;
    }
}

void
mkl_umath_@TYPE@_absolute(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
#if @SUPPORTED_BY_VML@
    if(IS_UNARY_CONT(@type@, @type@) &&
           dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD &&
           DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@)) ) {
        CHUNKED_VML_CALL2(v@s@Abs, dimensions[0], @type@, args[0], args[1]);
        /* v@s@Abs(dimensions[0], (@type@*) args[0], (@type@*) args[1]); */
    } else
#endif
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            const @type@ tmp = in1 > 0 ? in1 : -in1;
            /* add 0 to clear -0.0 */
            *((@type@ *)op1) = tmp + 0;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}

void
mkl_umath_@TYPE@_negative(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    {
        UNARY_LOOP {
            const @type@ in1 = *(@type@ *)ip1;
            *((@type@ *)op1) = -in1;
        }
    }
}

void
mkl_umath_@TYPE@_positive(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = +in1;
    }
}

void
mkl_umath_@TYPE@_sign(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /* Sign of nan is nan */
    UNARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = in1 > 0 ? 1 : (in1 < 0 ? -1 : (in1 == 0 ? 0 : in1));
    }
}

void
mkl_umath_@TYPE@_modf(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = modf@c@(in1, (@type@ *)op2);
    }
}

void
mkl_umath_@TYPE@_frexp(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP_TWO_OUT {
        const @type@ in1 = *(@type@ *)ip1;
        *((@type@ *)op1) = frexp@c@(in1, (int *)op2);
    }
}

void
mkl_umath_@TYPE@_ldexp(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const int in2 = *(int *)ip2;
        *((@type@ *)op1) = ldexp@c@(in1, in2);
    }
}

void
mkl_umath_@TYPE@_ldexp_long(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /*
     * Additional loop to handle npy_long integer inputs (cf. #866, #1633).
     * npy_long != npy_int on many 64-bit platforms, so we need this second loop
     * to handle the default integer type.
     */
    BINARY_LOOP {
        const @type@ in1 = *(@type@ *)ip1;
        const long in2 = *(long *)ip2;
        if (((int)in2) == in2) {
            /* Range OK */
            *((@type@ *)op1) = ldexp@c@(in1, ((int)in2));
        }
        else {
            /*
             * Outside npy_int range -- also ldexp will overflow in this case,
             * given that exponent has less bits than npy_int.
             */
            if (in2 > 0) {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MAX_INT);
            }
            else {
                *((@type@ *)op1) = ldexp@c@(in1, NPY_MIN_INT);
            }
        }
    }
}

#define mkl_umath_@TYPE@_true_divide mkl_umath_@TYPE@_divide

/**end repeat**/

/*
 *****************************************************************************
 **                           COMPLEX LOOPS                                 **
 *****************************************************************************
 */

#define CGE(xr,xi,yr,yi) ((xr > yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi >= yi))
#define CLE(xr,xi,yr,yi) ((xr < yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi <= yi))
#define CGT(xr,xi,yr,yi) ((xr > yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi > yi))
#define CLT(xr,xi,yr,yi) ((xr < yr && !isnan(xi) && !isnan(yi)) \
                          || (xr == yr && xi < yi))
#define CEQ(xr,xi,yr,yi) (xr == yr && xi == yi)
#define CNE(xr,xi,yr,yi) (xr != yr || xi != yi)

/**begin repeat
 * complex types
 * #TYPE = CFLOAT, CDOUBLE#
 * #ftype = npy_float, npy_double#
 * #c = f, #
 * #C = F, #
 * #s = s, d#
 * #SUPPORTED_BY_VML = 1, 1#
 */

/* similar to pairwise sum of real floats */
#if defined(__ICC) || defined(__INTEL_COMPILER)
#ifdef _MSC_VER
#pragma intel optimization_level 1
#else
#pragma intel optimization_level 2
#endif
#endif
static void
pairwise_sum_@TYPE@(@ftype@ *rr, @ftype@ * ri, char * a, npy_intp n,
                    npy_intp stride)
{
    assert(n % 2 == 0);
    if (n < 8) {
        npy_intp i;

        *rr = 0.;
        *ri = 0.;
        for (i = 0; i < n; i += 2) {
            *rr += *((@ftype@ *)(a + i * stride + 0));
            *ri += *((@ftype@ *)(a + i * stride + sizeof(@ftype@)));
        }
        return;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        @ftype@ r[8];

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = *((@ftype@ *)(a + 0 * stride));
        r[1] = *((@ftype@ *)(a + 0 * stride + sizeof(@ftype@)));
        r[2] = *((@ftype@ *)(a + 2 * stride));
        r[3] = *((@ftype@ *)(a + 2 * stride + sizeof(@ftype@)));
        r[4] = *((@ftype@ *)(a + 4 * stride));
        r[5] = *((@ftype@ *)(a + 4 * stride + sizeof(@ftype@)));
        r[6] = *((@ftype@ *)(a + 6 * stride));
        r[7] = *((@ftype@ *)(a + 6 * stride + sizeof(@ftype@)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512 /(npy_intp)sizeof(@ftype@))*stride, 0, 3);
        r[0] += *((@ftype@ *)(a + (i + 0) * stride));
            r[1] += *((@ftype@ *)(a + (i + 0) * stride + sizeof(@ftype@)));
        r[2] += *((@ftype@ *)(a + (i + 2) * stride));
            r[3] += *((@ftype@ *)(a + (i + 2) * stride + sizeof(@ftype@)));
        r[4] += *((@ftype@ *)(a + (i + 4) * stride));
            r[5] += *((@ftype@ *)(a + (i + 4) * stride + sizeof(@ftype@)));
        r[6] += *((@ftype@ *)(a + (i + 6) * stride));
            r[7] += *((@ftype@ *)(a + (i + 6) * stride + sizeof(@ftype@)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        *rr = ((r[0] + r[2]) + (r[4] + r[6]));
        *ri = ((r[1] + r[3]) + (r[5] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i+=2) {
            *rr += *((@ftype@ *)(a + i * stride + 0));
            *ri += *((@ftype@ *)(a + i * stride + sizeof(@ftype@)));
        }
        return;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        @ftype@ rr1, ri1, rr2, ri2;
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        pairwise_sum_@TYPE@(&rr1, &ri1, a, n2, stride);
        pairwise_sum_@TYPE@(&rr2, &ri2, a + n2 * stride, n - n2, stride);
        *rr = rr1 + rr2;
        *ri = ri1 + ri2;
        return;
    }
}

/**begin repeat1
 * arithmetic
 * #kind = add, subtract#
 * #OP = +, -#
 * #PW = 1, 0#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if (IS_BINARY_REDUCE && @PW@) {
        npy_intp n = dimensions[0];
        @ftype@ * or = ((@ftype@ *)args[0]);
        @ftype@ * oi = ((@ftype@ *)args[0]) + 1;
        @ftype@ rr, ri;

        pairwise_sum_@TYPE@(&rr, &ri, args[1], n * 2, steps[1] / 2);
        *or @OP@= rr;
        *oi @OP@= ri;
        return;
    }
    else {
        BINARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            const @ftype@ in2r = ((@ftype@ *)ip2)[0];
            const @ftype@ in2i = ((@ftype@ *)ip2)[1];
            ((@ftype@ *)op1)[0] = in1r @OP@ in2r;
            ((@ftype@ *)op1)[1] = in1i @OP@ in2i;
        }
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_multiply(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        ((@ftype@ *)op1)[0] = in1r*in2r - in1i*in2i;
        ((@ftype@ *)op1)[1] = in1r*in2i + in1i*in2r;
    }
}

void
mkl_umath_@TYPE@_divide(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        const @ftype@ in2r_abs = fabs@c@(in2r);
        const @ftype@ in2i_abs = fabs@c@(in2i);
        if (in2r_abs >= in2i_abs) {
            if (in2r_abs == 0 && in2i_abs == 0) {
                /* divide by zero should yield a complex inf or nan */
                ((@ftype@ *)op1)[0] = in1r/in2r_abs;
                ((@ftype@ *)op1)[1] = in1i/in2i_abs;
            }
            else {
                const @ftype@ rat = in2i/in2r;
                const @ftype@ scl = 1.0@c@/(in2r + in2i*rat);
                ((@ftype@ *)op1)[0] = (in1r + in1i*rat)*scl;
                ((@ftype@ *)op1)[1] = (in1i - in1r*rat)*scl;
            }
        }
        else {
            const @ftype@ rat = in2r/in2i;
            const @ftype@ scl = 1.0@c@/(in2i + in2r*rat);
            ((@ftype@ *)op1)[0] = (in1r*rat + in1i)*scl;
            ((@ftype@ *)op1)[1] = (in1i*rat - in1r)*scl;
        }
    }
}

/**begin repeat1
 * #kind= greater, greater_equal, less, less_equal, equal, not_equal#
 * #OP = CGT, CGE, CLT, CLE, CEQ, CNE#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = @OP@(in1r,in1i,in2r,in2i);
    }
}
/**end repeat1**/

/**begin repeat1
   #kind = logical_and, logical_or#
   #OP1 = ||, ||#
   #OP2 = &&, ||#
*/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = (in1r @OP1@ in1i) @OP2@ (in2r @OP1@ in2i);
    }
}
/**end repeat1**/

void
mkl_umath_@TYPE@_logical_xor(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        const npy_bool tmp1 = (in1r || in1i);
        const npy_bool tmp2 = (in2r || in2i);
        *((npy_bool *)op1) = tmp1 != tmp2;
    }
}

void
mkl_umath_@TYPE@_logical_not(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        *((npy_bool *)op1) = !(in1r || in1i);
    }
}

/**begin repeat1
 * #kind = isnan, isinf, isfinite#
 * #func = isnan, isinf, isfinite#
 * #OP = ||, ||, &&#
 **/
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        *((npy_bool *)op1) = @func@(in1r) @OP@ @func@(in1i);
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

void
mkl_umath_@TYPE@_square(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        ((@ftype@ *)op1)[0] = in1r*in1r - in1i*in1i;
        ((@ftype@ *)op1)[1] = in1r*in1i + in1i*in1r;
    }
}

void
mkl_umath_@TYPE@_reciprocal(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        if (fabs@c@(in1i) <= fabs@c@(in1r)) {
            const @ftype@ r = in1i/in1r;
            const @ftype@ d = in1r + in1i*r;
            ((@ftype@ *)op1)[0] = 1/d;
            ((@ftype@ *)op1)[1] = -r/d;
        } else {
            const @ftype@ r = in1r/in1i;
            const @ftype@ d = in1r*r + in1i;
            ((@ftype@ *)op1)[0] = r/d;
            ((@ftype@ *)op1)[1] = -1/d;
        }
    }
}

void
mkl_umath_@TYPE@__ones_like(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data))
{
    OUTPUT_LOOP {
        ((@ftype@ *)op1)[0] = 1;
        ((@ftype@ *)op1)[1] = 0;
    }
}

void
mkl_umath_@TYPE@_conjugate(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func)) {
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        ((@ftype@ *)op1)[0] = in1r;
        ((@ftype@ *)op1)[1] = -in1i;
    }
}

void
mkl_umath_@TYPE@_absolute(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    int ignore_fpstatus = 0;

    // FIXME: abs function VML for complex numbers breaks FFT test_basic.py
    //if(steps[0]/2 == sizeof(@ftype@) && steps[1] == sizeof(@ftype@) && dimensions[0] > VML_TRANSCEDENTAL_THRESHOLD) {
#if @SUPPORTED_BY_VML@
    if(0 == 1) {
        ignore_fpstatus = 1;
        CHUNKED_VML_CALL2(v@s@Abs, dimensions[0], @ftype@, args[0], args[1]);
        /* v@s@Abs(dimensions[0], (@ftype@ *) args[0], (@ftype@ *) args[1]); */
    } else
#endif
    {
        UNARY_LOOP {
            const @ftype@ in1r = ((@ftype@ *)ip1)[0];
            const @ftype@ in1i = ((@ftype@ *)ip1)[1];
            if(in1r == 0.0 && in1i == 0.0){
                ignore_fpstatus = 1;
            }
            *((@ftype@ *)op1) = hypot@c@(in1r, in1i);
        }
    }

    if(ignore_fpstatus) {
        feclearexcept(FE_DIVBYZERO | FE_OVERFLOW | FE_UNDERFLOW | FE_INVALID);
    }
}

void
mkl_umath_@TYPE@__arg(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        *((@ftype@ *)op1) = atan2@c@(in1i, in1r);
    }
}

void
mkl_umath_@TYPE@_sign(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    /* fixme: sign of nan is currently 0 */
    UNARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        ((@ftype@ *)op1)[0] = CGT(in1r, in1i, 0.0, 0.0) ?  1 :
                            (CLT(in1r, in1i, 0.0, 0.0) ? -1 :
                            (CEQ(in1r, in1i, 0.0, 0.0) ?  0 : NPY_NAN@C@));
        ((@ftype@ *)op1)[1] = 0;
    }
    feclearexcept(FE_INVALID);
}

/**begin repeat1
 * #kind = maximum, minimum#
 * #OP = CGE, CLE#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        @ftype@ in1r = ((@ftype@ *)ip1)[0];
        @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        if ( !(isnan(in1r) || isnan(in1i) || @OP@(in1r, in1i, in2r, in2i))) {
            in1r = in2r;
            in1i = in2i;
        }
        ((@ftype@ *)op1)[0] = in1r;
        ((@ftype@ *)op1)[1] = in1i;
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

/**begin repeat1
 * #kind = fmax, fmin#
 * #OP = CGE, CLE#
 */
void
mkl_umath_@TYPE@_@kind@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        if (isnan(in2r) || isnan(in2i) || @OP@(in1r, in1i, in2r, in2i)) {
            ((@ftype@ *)op1)[0] = in1r;
            ((@ftype@ *)op1)[1] = in1i;
        }
        else {
            ((@ftype@ *)op1)[0] = in2r;
            ((@ftype@ *)op1)[1] = in2i;
        }
    }
    feclearexcept(FE_ALL_EXCEPT); /* clear floatstatus */
}
/**end repeat1**/

#define mkl_umath_@TYPE@_true_divide mkl_umath_@TYPE@_divide

/**end repeat**/

#undef CGE
#undef CLE
#undef CGT
#undef CLT
#undef CEQ
#undef CNE

/*
 *****************************************************************************
 **                              END LOOPS                                  **
 *****************************************************************************
 */
